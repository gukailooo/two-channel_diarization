import subprocess
import wave
import numpy as np
from pathlib import Path
import os
import re
import difflib
import torch
import torchaudio
from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline

# === –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ PCM ===
def convert_to_pcm(input_file, output_file):
    cmd = [
        "ffmpeg", "-y", "-i", input_file,
        "-acodec", "pcm_s16le", "-ar", "44100", "-ac", "2", output_file
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ ===
def save_wav_channel(folder, wav, channel):
    nch = wav.getnchannels()
    depth = wav.getsampwidth()
    wav.setpos(0)
    sdata = wav.readframes(wav.getnframes())
    typ = {1: np.uint8, 2: np.int16, 4: np.int32}.get(depth)
    if channel >= nch:
        raise ValueError(f"–ö–∞–Ω–∞–ª {channel+1} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
    data = np.frombuffer(sdata, dtype=typ)
    ch_data = data[channel::nch]
    out_path = Path(folder) / f"speaker{channel+1}.wav"
    with wave.open(str(out_path), 'wb') as outwav:
        outwav.setparams((1, depth, wav.getframerate(), 0, 'NONE', 'not compressed'))
        outwav.writeframes(ch_data.tobytes())

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–æ–Ω–æ ===
def is_mono_audio(file1, file2, tolerance=1e-3):
    y1, sr1 = torchaudio.load(file1)
    y2, sr2 = torchaudio.load(file2)
    if sr1 != sr2:
        return False
    min_len = min(y1.shape[-1], y2.shape[-1])
    y1, y2 = y1[..., :min_len], y2[..., :min_len]
    y1 = y1.mean(dim=0)
    y2 = y2.mean(dim=0)
    corr = torch.corrcoef(torch.stack([y1, y2]))[0,1].item()
    diff = torch.mean(torch.abs(y1 - y2)).item()
    return corr > 0.995 and diff < tolerance

# === –û—á–∏—Å—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑ –ø–æ —Å–º—ã—Å–ª—É ===
def clean_repeated_phrases(segments, similarity_threshold=0.85):
    cleaned = []
    for start, end, speaker, text in segments:
        text = text.strip()
        sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]
        new_sentences = []
        last_sentence = ""
        for s in sentences:
            sim = difflib.SequenceMatcher(None, last_sentence, s).ratio()
            if sim < similarity_threshold:
                new_sentences.append(s)
                last_sentence = s
        cleaned_text = '. '.join(new_sentences)

        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥
        cleaned_text = re.sub(r'\b(\w+)(?:\s+\1\b){1,}', r'\1', cleaned_text, flags=re.IGNORECASE)

        # –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è/–∑–∞–º–µ–Ω—ã
        patterns = [
            r'\b(–¥–∞|–∞–≥–∞|—É–≥—É|–ø–æ–Ω—è–ª–∞|–ø–æ–Ω—è–ª|–Ω—É|–≤–æ—Ç|—ç—Ç–æ|—Å–µ–π—á–∞—Å|—Ç–∞–∫|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ)\b(?:\s+\1\b){1,}',
            r'—Ä–µ–¥–∞–∫—Ç–æ—Ä\s*—Å—É–±—Ç–∏—Ç—Ä\w*\s*[–∞a]\.?[\s\w]*—Å–µ–º–∫–∏–Ω',
            r'–∫–æ—Ä—Ä–µ–∫—Ç–æ—Ä\s*[–∞a]\.?[\s\w]*–µ–≥–æ—Ä–æ–≤',
            r'–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç',
            r'–∑–≤–æ–Ω–æ–∫ –≤ –¥–≤–µ—Ä—å',
            r'—Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–π –∑–≤–æ–Ω–æ–∫'
        ]
        for pat in patterns:
            cleaned_text = re.sub(pat, '', cleaned_text, flags=re.IGNORECASE)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned_text = re.sub(r'\s{2,}', ' ', cleaned_text).strip()

        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ö–≤–æ—Å—Ç
        words = cleaned_text.split()
        if len(words) > 10:
            tail = " ".join(words[-6:])
            if cleaned_text.count(tail) > 1:
                cleaned_text = cleaned_text.replace(tail, "", cleaned_text.count(tail) - 1).strip()

        if cleaned_text:
            cleaned.append((start, end, speaker, cleaned_text))
    return cleaned


# === –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å —Ç–∞–π–º–∫–æ–¥–∞–º–∏ ===
def transcribe_with_timestamps(audio_path, speaker_name, asr):
    waveform, sr = torchaudio.load(audio_path)
    if sr != 16000:
        waveform = torchaudio.functional.resample(waveform, sr, 16000)
        sr = 16000
    if waveform.shape[0] > 1:
        waveform = torch.mean(waveform, dim=0, keepdim=True)

    temp_path = f"temp_{speaker_name}.wav"
    torchaudio.save(temp_path, waveform, sr)
    result = asr(temp_path, chunk_length_s=25, return_timestamps=True)
    segments = []
    for chunk in result["chunks"]:
        start, end = chunk["timestamp"]
        start = start if start is not None else 0.0
        end = end if end is not None else start + 0.5
        text = chunk["text"].strip()
        if text:
            segments.append((start, end, speaker_name, text))
    os.remove(temp_path)
    segments = clean_repeated_phrases(segments)
    return segments

# === –°–ª–∏—è–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–≤—É—Ö –≥–æ–≤–æ—Ä—è—â–∏—Ö ===
def merge_dialog_segments(segments1, segments2, overlap_tolerance=1.0):
    merged = sorted(segments1 + segments2, key=lambda x: x[0])
    cleaned = []

    for seg in merged:
        start, end, speaker, text = seg
        if cleaned and start - cleaned[-1][1] < overlap_tolerance and speaker == cleaned[-1][2]:
            combined_text = cleaned[-1][3] + " " + text
            cleaned[-1] = (cleaned[-1][0], max(end, cleaned[-1][1]), speaker, combined_text)
        else:
            cleaned.append(seg)

    cleaned = clean_repeated_phrases(cleaned)
    return cleaned

# === –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ txt ===
def extract_metadata(txt_path):
    with open(txt_path, "r", encoding="utf-8") as f:
        text = f.read()
    date = re.search(r"(\d{2}_\d{2}_\d{4})", text)
    time = re.search(r"(\d{2}:\d{2}:\d{2})", text)
    abonent = re.search(r"–û–±—ä–µ–∫—Ç\s*[-‚Äì]?\s*(\d+)", text)
    contact = re.search(r"–ê–±–æ–Ω–µ–Ω—Ç\s*[-‚Äì]?\s*(\d+)", text)
    return {
        "date": date.group(1) if date else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
        "time": time.group(1) if time else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
        "abonent": abonent.group(1) if abonent else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
        "contact": contact.group(1) if contact else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    }

# === –û—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫ ===
if __name__ == "__main__":
    target_numbers = []

    input_root = Path(input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ñ–∞–π–ª–∞–º–∏: ").strip())
    output_root = input_root / "results"
    request_root = output_root / "request"
    other_root = output_root / "other"
    request_root.mkdir(parents=True, exist_ok=True)
    other_root.mkdir(parents=True, exist_ok=True)

    request_summary_path = request_root / "all_request.txt"
    other_summary_path = other_root / "all_other.txt"

    model_name = "C:/Users/trans/Desktop/diar/models/whisper-large-v3-turbo"
    device = 0 if torch.cuda.is_available() else -1
    model = WhisperForConditionalGeneration.from_pretrained(model_name).to("cuda" if device==0 else "cpu")
    processor = WhisperProcessor.from_pretrained(model_name)
    asr = pipeline(
        task="automatic-speech-recognition",
        model=model,
        tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor,
        device=device,
        generate_kwargs={"temperature":0.0,"no_repeat_ngram_size":4}
    )

    for txt_file in input_root.rglob("*.txt"):
        base_name = txt_file.stem
        audio_path = None
        for ext in [".wav", ".mp3", ".flac"]:
            candidate = txt_file.with_suffix(ext)
            if candidate.exists():
                audio_path = candidate
                break
        if not audio_path:
            continue

        meta = extract_metadata(txt_file)
        abonent = meta["abonent"]
        contact = meta["contact"]
        time_clean = meta["time"].replace(":", "-") if meta["time"] != "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ" else "00-00-00"
        date = meta["date"]
        folder_name = f"{date}-({time_clean})-{contact}"

        category_root = request_root if abonent in target_numbers or contact in target_numbers else other_root
        category_summary_path = request_summary_path if category_root==request_root else other_summary_path

        contact_folder = category_root / contact / folder_name
        contact_folder.mkdir(parents=True, exist_ok=True)
        pcm_file = contact_folder / f"{base_name}_pcm.wav"
        convert_to_pcm(audio_path, str(pcm_file))

        wav = wave.open(str(pcm_file), 'rb')
        for ch in range(wav.getnchannels()):
            save_wav_channel(contact_folder, wav, ch)
        wav.close()

        file1 = str(contact_folder / "speaker1.wav")
        file2 = str(contact_folder / "speaker2.wav")

        if os.path.exists(file2) and not is_mono_audio(file1, file2):
            segments1 = transcribe_with_timestamps(file1, "–ê–±–æ–Ω–µ–Ω—Ç_–ê", asr)
            segments2 = transcribe_with_timestamps(file2, "–ê–±–æ–Ω–µ–Ω—Ç_–ë", asr)
            all_segments = merge_dialog_segments(segments1, segments2)
        else:
            all_segments = transcribe_with_timestamps(file1, "–ê–±–æ–Ω–µ–Ω—Ç", asr)

        dialogue_path = contact_folder / f"dialogue_{date}-({time_clean})-{contact}.txt"
        with open(dialogue_path, "w", encoding="utf-8") as f:
            f.write(f"–î–∞—Ç–∞: {meta['date']}\n–í—Ä–µ–º—è: {meta['time']}\n–ê–±–æ–Ω–µ–Ω—Ç: {meta['abonent']}\n–ö–æ–Ω—Ç–∞–∫—Ç: {meta['contact']}\n\n")
            for _, _, speaker, text in all_segments:
                f.write(f"{speaker}: {text}\n")

        with open(category_summary_path, "a", encoding="utf-8") as f:
            f.write(f"\n–î–∞—Ç–∞: {meta['date']}\n–í—Ä–µ–º—è: {meta['time']}\n–ê–±–æ–Ω–µ–Ω—Ç: {meta['abonent']}\n–ö–æ–Ω—Ç–∞–∫—Ç: {meta['contact']}\n\n")
            for _, _, speaker, text in all_segments:
                f.write(f"{speaker}: {text}\n")
            f.write("="*80 + "\n")

        contact_summary_path = contact_folder.parent / f'{contact}_all.txt'
        with open(contact_summary_path, 'a', encoding='utf-8') as f:
            f.write(f"\n–î–∞—Ç–∞: {meta['date']}\n–í—Ä–µ–º—è: {meta['time']}\n–ê–±–æ–Ω–µ–Ω—Ç: {meta['abonent']}\n–ö–æ–Ω—Ç–∞–∫—Ç: {meta['contact']}\n\n")
            for _, _, speaker, text in all_segments:
                f.write(f"{speaker}: {text}\n")
            f.write("\n" + '-'*100 + "\n" + "="*80 + "\n")

    print("\nüìò –í—Å–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø–æ –Ω–æ–º–µ—Ä–∞–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.")
